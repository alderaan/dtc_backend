# DTC Backend

This directory contains the backend infrastructure for the DTC (Direct-to-Consumer) scraping and data processing system.

## Overview

The backend consists of several components:
- **dbt**: Data transformation and modeling
- **Supabase**: Database and API layer
- **n8n**: Workflow automation

## DBT (Data Build Tool)

### What is DBT?

DBT (Data Build Tool) is an open-source tool that enables data analysts and engineers to transform data in their warehouse more effectively. It allows you to write SQL queries as models and run them in a specific order to build your data pipeline.

### Project Structure

```
dtc_dbt/
├── models/
│   ├── staging/          # Raw data cleaning and preparation
│   │   ├── stg_google_search_results.sql
│   │   ├── stg_google_search_organic_results.sql
│   │   └── schema.yml
│   └── marts/           # Business-level transformations
│       ├── google_search_instagram_profiles.sql
│       ├── dtc_profiles.sql
│       └── dtc_profile_keywords.sql
├── packages.yml         # External dependencies
├── dbt_project.yml      # Project configuration
└── profiles/           # Database connection settings
```

### Data Pipeline Overview

Our DBT pipeline transforms raw Google search results into structured Instagram profile data:

1. **Raw Data**: Google search results stored as JSON in `public.dtc_raw_google_search_results`
2. **Staging**: Clean and prepare raw data
3. **Marts**: Create business-ready tables for analysis

### Pipeline Steps

#### Step 1: Raw Data Source
- **Table**: `public.dtc_raw_google_search_results`
- **Content**: Raw JSON data from Google search scraper
- **Structure**: Each row represents one keyword search with JSON results in the `data` column

#### Step 2: Staging Models
- **Purpose**: To clean and prepare the raw data for transformation.
- **Models**:
    - `stg_google_search_results`: Takes the raw source and renames columns for clarity.
    - `stg_google_search_organic_results`: Expands the `organicResults` JSON array into individual rows and filters for only Instagram URLs.
- **Materialization**: These are built as `views` for efficiency, meaning they are just saved queries and do not store data themselves.

#### Step 3: Marts Models
- **Purpose**: To create the final, structured tables for business use.
- **Models**:
    - `google_search_instagram_profiles`: An intermediate model that parses the Instagram URLs from the staging layer to extract usernames.
    - `dtc_profiles`: The final table for unique Instagram profiles. It generates a unique, deterministic `id` (a surrogate key) for each username.
    - `dtc_profile_keywords`: A junction table that links each profile to the search keyword that discovered it.
- **Materialization**: These are built as `incremental` tables. This means dbt will only insert or update new records on subsequent runs, making the process much faster.

### Key Concepts

- **Surrogate Keys**: Instead of relying on the database's auto-incrementing integers, we generate our own unique IDs using the `dbt_utils.generate_surrogate_key()` macro. This makes our models self-contained, robust, and idempotent.
- **Incremental Models**: This materialization strategy is key to efficient data processing. On the first run, the table is built fully. On subsequent runs, only new rows are added, which is significantly faster than a full rebuild.

### How to Run

1.  **Install Dependencies**: From the `dtc_dbt` directory, run `dbt deps` to install packages like `dbt_utils`.
2.  **Run the Pipeline**: From the `dtc_dbt` directory, run `dbt run`.
    -   To force a full rebuild of all models from scratch, run `dbt run --full-refresh`. This is useful if a model's logic has changed or data has become corrupted.
3.  **Test Data Quality**: Run `dbt test` to execute any data tests defined in the `schema.yml` files (e.g., checking for nulls or duplicates).

---

### Managing Search Terms via Seeds and Snapshots

To ensure our list of search terms is version-controlled and to maintain historical integrity, we use a combination of dbt Seeds and Snapshots.

- **Seeds**: The `dtc_search_terms.csv` file in the `seeds` directory is the single source of truth for what search terms are currently active.
- **Snapshots**: dbt snapshots are used to create a historical record of this data. This allows us to see what search terms were active at any point in time, even after they have been removed from the seed file.

This is the standard dbt methodology for handling "Slowly Changing Dimensions."

#### Schema Design for Seeds and Snapshots

- **`dtc_seeds`**: Contains the live, current version of the search terms table, loaded directly from the CSV.
- **`dtc_snapshots`**: Contains the historical, timestamped version of the search terms table. Rows in this schema are never deleted.

#### Joining to Search Terms
The unique identifier for each search term is **`search_term_id`**. This ID is a stable hash generated by the snapshot process. To ensure you are linking to the correct historical version of a search term, you should join your models on `search_term_id` from the `snapshot_search_terms` table.

#### Workflow for Updating Search Terms

Whenever you need to add, remove, or edit a search term, you must follow these steps:

1.  **Edit the CSV File**: Make your changes directly in `dtc_backend/dtc_dbt/seeds/dtc_search_terms.csv`.
2.  **Commit the Change**: Commit the updated CSV file to your git repository. This is a crucial step for maintaining an auditable log.
3.  **Run the dbt Commands**: To correctly apply your changes to the data warehouse, you must run the following dbt commands **in this specific order**. These commands should be run from the `~/dbt` directory where your `docker-compose.yml` is located.

    1.  **Load the Seed Data:** This command updates the table in the `dtc_seeds` schema to match the current state of your CSV file.
        ```bash
        docker compose run --rm dbt seed
        ```
    2.  **Capture the Snapshot:** This command compares the newly seeded table to the last historical record and logs any changes.
        ```bash
        docker compose run --rm dbt snapshot
        ```
    3.  **Run Your Models:** This command runs all your regular transformations, which can now safely reference the updated historical data.
        ```bash
        docker compose run --rm dbt run
        ``` 